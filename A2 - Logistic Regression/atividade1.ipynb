{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression using Deep Learning\n",
    "\n",
    "## Proposed problem\n",
    "For this activity we will create a prediction model to classify if the patient tend to have a heart condition or not. Implement a logistic regression using a basic neural network with ReLU and Sigmoid activation functions. It is important to understand the role of nonlinearity on neural network architectures. Build your model testing different numbers of neurons and hidden layers to achieve best results. This task should be done using 'pytorch'. For this activity, use the dataset from Kaegle listed on the Dataset section. Note that all columns shall be used (more explanetion of the dataset is avaliable on Kaggle). See the Support Material section for guidelines for studies.\n",
    "\n",
    "Finally, write a report with the results. For this report use the LaTeX template present on this repo ('.tex' and '.bib' files).\n",
    "\n",
    "\n",
    "## Dataset\n",
    "- https://www.kaggle.com/ronitf/heart-disease-uci\n",
    "\n",
    "## Support Material/References\n",
    "### Logistic Regression:\n",
    "\t\n",
    "- [Professor Andrew NG - Logistic Regression:](https://www.youtube.com/watch?v=-la3q9d7AKQ)\n",
    "\t\n",
    "- [Pytorch Engineer - Pytorch:](https://www.youtube.com/watch?v=OGpQxIkR4ao&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=8)\n",
    "\t \n",
    "- [Nonlinearities and activation functions:](https://www.deeplearningbook.com.br/funcao-de-ativacao/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "xy = pd.read_csv(\"_data/heart.csv\")\n",
    "\n",
    "# display dataset as a pandas dataframe\n",
    "xy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separete feature (x) and labels (y)\n",
    "x = xy.iloc[:,0:-1] # all columns but the last\n",
    "y = xy.iloc[:,-1] # last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Verify if there is any missing data\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify if data is imbalanced\n",
    "num_0 = 0\n",
    "num_1 = 0\n",
    "for i in range(len(y)):\n",
    "    if (y[i] == 1):\n",
    "        num_1 += 1\n",
    "    elif(y[i] == 0):\n",
    "        num_0 += 1\n",
    "\n",
    "if(num_0 != num_1):\n",
    "    print(\"Data is imbalanced!\")\n",
    "else:\n",
    "    print(\"Data is balanced!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What is the difference between the training, validation and test datset? What each one is used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: split into train, validation and test sets\n",
    "\n",
    "''' hint: Use stratfy from sklearn \n",
    "(https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) \n",
    "\n",
    "We used stratify because the sample is imbalanced (the number of examples in each class is not equal).\n",
    "Thus, stratification will help to ensure that training and test data have the same distribution of classes.\n",
    "'''\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalize the input (x in [0,1])\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Preprocess data to create dataloaders\n",
    "class HeartDataset(Dataset):\n",
    "    ''' Receive a dataset and preprocess to transform into data loaders\n",
    "    \n",
    "        Input:\n",
    "            x_data (float numpy array): input values\n",
    "            y_data (float numpy array): expected output value\n",
    "    '''\n",
    "    def __init__(self, x_data, y_data):\n",
    "        # TODO: complete the generator\n",
    "        \n",
    "        # YOUR CODE HERE (delete pass after filling with your code)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "train_dataset = HeartDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train).long())\n",
    "val_dataset = HeartDataset(torch.from_numpy(x_val).float(), torch.from_numpy(y_val).long())\n",
    "test_dataset = HeartDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure that all of our mini-batches see all of our classes, we will need to over-sample\n",
    "# the classes with few number of examples. For that, we will use the WeightRandomSampler.\n",
    "# In this way, imblanced data can be handler and overfitting prevented.\n",
    "\n",
    "target_list = [] # Contain all the outputs\n",
    "for _, t in train_dataset:\n",
    "    target_list.append(t)\n",
    "\n",
    "target_list = torch.tensor(target_list) # Convert to tensor\n",
    "target_list = target_list[torch.randperm(len(target_list))] # shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize with random weights to accelerate convergence\n",
    "\n",
    "def get_class_distribution(obj):\n",
    "    '''\n",
    "    Input obj (data structure): receive the data regarding to labels\n",
    "    \n",
    "    Output: class as key and their count as values\n",
    "    '''\n",
    "    count_dict = {\n",
    "        \"rating_0\": 0,\n",
    "        \"rating_1\": 0,\n",
    "    }\n",
    "    \n",
    "    for i in obj:\n",
    "        if i == 0: \n",
    "            count_dict['rating_0'] += 1\n",
    "        elif i == 1: \n",
    "            count_dict['rating_1'] += 1             \n",
    "        else:\n",
    "            print(\"Check classes.\")\n",
    "            \n",
    "    return count_dict\n",
    "\n",
    "class_count = [i for i in get_class_distribution(y_train).values()]\n",
    "class_weights = 1./torch.tensor(class_count, dtype=torch.float)\n",
    "\n",
    "\n",
    "class_weights_all = class_weights[target_list]\n",
    "weighted_sampler = WeightedRandomSampler(weights=class_weights_all,\n",
    "num_samples=len(class_weights_all), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining hyperparameters\n",
    "num_epochs = 50\n",
    "batch_size = 16\n",
    "learning_rate = 1e-6\n",
    "\n",
    "input_size = len(x.columns) # Number of features\n",
    "num_classes = 2 # Number of classes (0 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What is the difference between epochs, batch size and number of iterations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: building the dataloaders \n",
    "# hint: don't forget to use the weighted_sampler as sampler\n",
    "# https://pytorch.org/docs/stable/data.html\n",
    "\n",
    "# TODO: complete the dataloader (see https://pytorch.org/docs/stable/data.html)\n",
    "train_loader = None # YOUR CODE HERE\n",
    "val_loader = None # YOUR CODE HERE\n",
    "test_loader = None # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: build the neural network architecture for linear regression\n",
    "# hint: don't forget to add the sigmoid activation function at the output\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # TODO: complete with the Neural Network architecture (set up the linear layer)\n",
    "        \n",
    "        # YOUR CODE HERE (delete pass after filling with your code)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: complete the forward method\n",
    "        \n",
    "        # YOUR CODE HERE (delete pass after filling with your code)\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the method\n",
    "model = None # YOUR CODE HERE\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    " \n",
    "# TODO: complete with the loss function (hint: use Binary Cross Entropy - BCE) \n",
    "criterion = None # YOUR CODE HERE\n",
    "\n",
    "# TODO: complete with the optimizer (hint: use Adam Optimizer)\n",
    "optimizer = None # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training blackbox\n",
    "print(\"Begin training.\")\n",
    "for e in tqdm(range(1, num_epochs+1)):\n",
    "    \n",
    "    # monitoring variables \n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: TRAINING PIPELINE\n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        \n",
    "        # alloacate to GPU or CPU\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        \n",
    "        # clean the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # TODO: coomplete the forward pass\n",
    "        y_train_pred = None # Your code here\n",
    "        \n",
    "        # dimension fix\n",
    "        y_train_batch= y_train_batch.type(torch.FloatTensor).to(device).unsqueeze(1)\n",
    "        \n",
    "        # clean the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # TODO: complete the forward pass\n",
    "        train_loss = None # Your code here\n",
    "        \n",
    "        # train accuracy metric calculation\n",
    "        train_acc = accuracy_score(y_train_pred.cpu().detach().numpy().round(), y_train_batch.cpu().detach().numpy(), normalize=False)\n",
    "        \n",
    "        # TODO: complete the backward pass\n",
    "        # Your code here \n",
    "        \n",
    "        # TODO: complete the weight update\n",
    "        # Your code here\n",
    "        \n",
    "        # updating loss and epoch accuracy values\n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "\n",
    "\n",
    "    # VALIDATION PIPELINE\n",
    "    with torch.no_grad(): # do not use gradients here\n",
    "        \n",
    "        # monitoring variables\n",
    "        val_epoch_loss = 0\n",
    "        val_epoch_acc = 0\n",
    "        \n",
    "        # model evaluation\n",
    "        model.eval()\n",
    "        \n",
    "        # validation iteration for each batch\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            \n",
    "            # alloacate to GPU or CPU\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            y_val_pred = model(X_val_batch)\n",
    "            \n",
    "            # dimension fix\n",
    "            y_val_batch = y_val_batch.type(torch.FloatTensor).to(device).unsqueeze(1)\n",
    "            \n",
    "            # loss calculation\n",
    "            val_loss = criterion(y_val_pred, y_val_batch)\n",
    "            \n",
    "            # validation accuracy metric calculation\n",
    "            val_acc = accuracy_score(y_val_pred.cpu().detach().numpy().round(), y_val_batch.cpu().detach().numpy(), normalize=False)\n",
    "            \n",
    "            # updating loss and epoch accuracy values\n",
    "            val_epoch_loss += val_loss.item()\n",
    "            val_epoch_acc += val_acc.item()\n",
    "\n",
    "    \n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | Train Acc: {train_epoch_acc/len(train_dataset):.3f}| Val Acc: {val_epoch_acc/len(val_dataset):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing blackbox\n",
    "\n",
    "# monitoring variable \n",
    "test_epoch_acc = 0\n",
    "\n",
    "with torch.no_grad(): # do not use gradients here\n",
    "    \n",
    "    # model evaluation\n",
    "    model.eval()\n",
    "    \n",
    "    # test iteration\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        \n",
    "        # alloacate to GPU or CPU\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # compute the prediction using the trained model\n",
    "        y_test_pred = model(X_batch)\n",
    "        \n",
    "        # test accuracy metric calculation\n",
    "        test_acc = accuracy_score(y_test_pred.cpu().detach().numpy().round(), y_val_batch.cpu().detach().numpy(), normalize=False)\n",
    "        \n",
    "        # updating loss and epoch accuracy values\n",
    "        test_epoch_acc += test_acc.item()\n",
    "        \n",
    "print(f' Number of samples: {len(test_loader)} Test Score: {test_epoch_acc:.3f} Test Acc: {test_epoch_acc/len(test_dataset):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
